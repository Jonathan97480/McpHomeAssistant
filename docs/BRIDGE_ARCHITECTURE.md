# ðŸ—ï¸ ARCHITECTURE HTTP-MCP BRIDGE AVEC QUEUE

## ðŸ“ **VUE D'ENSEMBLE TECHNIQUE**

Le bridge HTTP-MCP transforme le protocole stdio MCP en API REST avec gestion avancÃ©e des files d'attente pour garantir la sÃ©curitÃ© des opÃ©rations concurrentes.

## ðŸ§© **COMPOSANTS PRINCIPAUX**

### **1. Queue Manager**
```python
class AsyncRequestQueue:
    """
    File d'attente FIFO thread-safe pour Ã©viter les conflits
    """
    - asyncio.Queue : File d'attente principale
    - Request deduplication : Ã‰vite les doublons
    - Priority levels : Support des prioritÃ©s
    - Timeout handling : Gestion des timeouts
    - Circuit breaker : Protection overload
```

### **2. Session Pool Manager**
```python
class MCPSessionPool:
    """
    Pool de connexions MCP rÃ©utilisables
    """
    - Session lifecycle : CrÃ©ation/destruction automatique
    - Health monitoring : Surveillance des sessions
    - Auto-recovery : Reconnexion automatique
    - Load balancing : RÃ©partition des requÃªtes
```

### **3. HTTP-MCP Bridge Core**
```python
class HTTPMCPBridge:
    """
    CÅ“ur du bridge HTTP â†” MCP
    """
    - JSON-RPC parsing : Conversion HTTP â†” JSON-RPC
    - Request routing : Routage vers les handlers
    - Response formatting : Formatage des rÃ©ponses
    - Error handling : Gestion d'erreurs robuste
```

---

## ðŸ”„ **FLUX DE DONNÃ‰ES**

### **ScÃ©nario Standard**
```
1. Client HTTP â†’ POST /mcp/tools/call
2. Request Queue â† Ajout requÃªte en file
3. Queue Manager â†’ Traitement FIFO
4. Session Pool â†’ Attribution session MCP disponible
5. MCP Session â† Envoi JSON-RPC via stdio
6. Home Assistant â† ExÃ©cution commande
7. MCP Session â†’ RÃ©ception rÃ©ponse
8. HTTP Response â† Formatage et envoi au client
```

### **ScÃ©nario Concurrent - GESTION DES CONFLITS**
```
Client A â†’ Request A â†’ Queue [A] â†’ Session 1 â†’ Processing A
Client B â†’ Request B â†’ Queue [A,B] â†’ Wait...
Client C â†’ Request C â†’ Queue [A,B,C] â†’ Wait...

Session 1 free â†’ Queue [B,C] â†’ Session 1 â†’ Processing B
Session 2 available â†’ Queue [C] â†’ Session 2 â†’ Processing C
```

---

## ðŸ“Š **GESTION AVANCÃ‰E DES FILES D'ATTENTE**

### **StratÃ©gie Anti-Collision**
```python
class CollisionAvoidance:
    """
    PrÃ©vention des conflits de requÃªtes simultanÃ©es
    """
    
    # MÃ©thode 1: SÃ©rialisation par entitÃ©
    entity_locks = {
        "light.salon": asyncio.Lock(),
        "switch.cuisine": asyncio.Lock()
    }
    
    # MÃ©thode 2: Queue par domaine
    domain_queues = {
        "light": AsyncQueue(),
        "switch": AsyncQueue(),
        "sensor": AsyncQueue()  # Read-only, pas de lock
    }
    
    # MÃ©thode 3: Session dÃ©diÃ©e par client
    client_sessions = {
        "session_id_1": MCPSession(),
        "session_id_2": MCPSession()
    }
```

### **Types de RequÃªtes & PrioritÃ©s**
```python
class RequestPriority(Enum):
    CRITICAL = 0  # get_entity_state (monitoring critique)
    HIGH = 1      # get_entities (lecture)
    MEDIUM = 2    # call_service (Ã©criture simple)
    LOW = 3       # create_automation (Ã©criture complexe)
    BULK = 4      # get_history (requÃªtes lourdes)
    
class RequestType(Enum):
    READ_ONLY = "read"      # Pas de conflit possible
    WRITE_SAFE = "write"    # Ã‰criture sans conflit
    WRITE_CRITICAL = "critical"  # Ã‰criture critique (besoin lock)
```

---

## ðŸ›¡ï¸ **SÃ‰CURITÃ‰ & ROBUSTESSE**

### **Protection Contre Surcharge**
```python
class OverloadProtection:
    # Rate limiting par client IP
    rate_limiter = TokenBucket(
        capacity=100,      # 100 requÃªtes max
        refill_rate=10     # 10 req/sec refill
    )
    
    # Circuit breaker global
    circuit_breaker = CircuitBreaker(
        failure_threshold=5,    # 5 Ã©checs consÃ©cutifs = ouverture
        recovery_timeout=30     # 30sec avant retry
    )
    
    # Request timeout configurable
    timeouts = {
        "get_entities": 10,        # 10sec max
        "call_service": 30,        # 30sec max
        "create_automation": 60    # 60sec max
    }
```

### **Gestion d'Erreurs AvancÃ©e**
```python
class ErrorHandling:
    # Retry automatique avec backoff exponentiel
    retry_config = {
        "max_attempts": 3,
        "backoff_factor": 2,
        "retry_exceptions": [ConnectionError, TimeoutError, MCPError]
    }
    
    # Fallback responses pour haute disponibilitÃ©
    fallback_strategies = {
        "get_entities": "return_cached_or_empty",
        "get_services": "return_cached_or_minimal",
        "call_service": "fail_fast_with_reason"
    }
```

---

## ðŸš€ **OPTIMISATIONS PERFORMANCE**

### **Cache Strategy Multi-Niveau**
```python
class IntelligentCache:
    # Cache L1 : MÃ©moire ultra-rapide
    l1_cache = LRUCache(maxsize=1000, ttl=60)
    
    # Cache L2 : Redis optionnel pour cluster
    l2_cache = RedisCache(ttl=300) if REDIS_ENABLED else None
    
    # Configuration par endpoint
    cache_policies = {
        "tools/list": {
            "ttl": 3600,           # 1h - les outils changent rarement
            "level": "l1", 
            "invalidate_on": ["server_restart"]
        },
        "get_entities": {
            "ttl": 60,             # 1min - Ã©tat change souvent
            "level": "l1",
            "invalidate_on": ["entity_state_change"]
        },
        "get_entity_state": {
            "ttl": 10,             # 10sec - temps rÃ©el
            "level": "l1",
            "vary_by": ["entity_id"]
        },
        "call_service": {
            "ttl": 0,              # Jamais de cache pour les actions
            "level": "none"
        }
    }
```

### **Connection Pooling Intelligent**
```python
class SmartConnectionPool:
    # Configuration adaptative
    pool_config = {
        "min_connections": 2,           # Minimum garanti
        "max_connections": 10,          # Maximum autorisÃ©
        "target_connections": 5,        # Cible optimale
        "scale_factor": 1.5,           # Facteur d'adaptation
        "idle_timeout": 300,           # 5min timeout inactivitÃ©
        "health_check_interval": 60,   # Check santÃ© 1min
        "auto_scale": True             # Adaptation automatique
    }
    
    # MÃ©triques pour auto-scaling
    def should_scale_up(self) -> bool:
        return (
            self.queue_size > self.active_connections * 2 and
            self.avg_response_time > 1000  # 1sec
        )
```

---

## ðŸ“¡ **API ENDPOINTS DÃ‰TAILLÃ‰S**

### **Core MCP Bridge**
```http
# Initialisation session
POST /mcp/initialize
Content-Type: application/json
{
    "protocolVersion": "2024-11-05",
    "capabilities": {"supports_progress": true},
    "clientInfo": {"name": "http-client", "version": "1.0"},
    "session_id": "optional-custom-id"
}
Response: {
    "jsonrpc": "2.0",
    "id": 1,
    "result": {
        "protocolVersion": "2024-11-05",
        "capabilities": {...},
        "serverInfo": {...},
        "session_id": "generated-or-custom-id"
    }
}

# Liste des outils avec cache
POST /mcp/tools/list
Headers: {
    "X-Session-ID": "session-id",
    "X-Cache-Control": "max-age=3600"
}
Response: {
    "tools": [...],
    "cached": true,
    "cache_expires": "2024-09-21T15:30:00Z"
}

# ExÃ©cution avec prioritÃ©
POST /mcp/tools/call
Headers: {
    "X-Session-ID": "session-id",
    "X-Priority": "HIGH",
    "X-Timeout": "30"
}
Body: {
    "name": "get_entities",
    "arguments": {"domain": "light"},
    "request_id": "unique-request-id"
}
```

### **Management & Monitoring AvancÃ©**
```http
# Statut dÃ©taillÃ© du bridge
GET /mcp/status
Response: {
    "bridge": {
        "status": "healthy",
        "version": "1.0.0",
        "uptime_seconds": 86400
    },
    "sessions": {
        "active": 3,
        "total": 5,
        "by_status": {"healthy": 3, "reconnecting": 1, "idle": 1}
    },
    "queue": {
        "pending": 2,
        "processing": 1,
        "by_priority": {"HIGH": 1, "MEDIUM": 2, "LOW": 0}
    },
    "cache": {
        "hit_rate": 0.85,
        "size": 150,
        "evictions": 10
    },
    "performance": {
        "avg_response_time_ms": 250,
        "requests_per_second": 15.5,
        "error_rate": 0.02
    }
}

# MÃ©triques Prometheus
GET /metrics
Response: Prometheus format avec mÃ©triques custom

# Health check avec dÃ©pendances
GET /health
Response: {
    "status": "healthy",
    "timestamp": "2024-09-21T14:30:00Z",
    "dependencies": {
        "home_assistant": {"status": "ok", "response_time_ms": 45},
        "mcp_sessions": {"status": "ok", "healthy_count": 3}
    }
}
```

---

## ðŸ”§ **CONFIGURATION AVANCÃ‰E**

### **Configuration ComplÃ¨te**
```yaml
# config/bridge.yaml
bridge:
  host: "0.0.0.0"
  port: 3003
  workers: 4
  debug: false
  
queue:
  max_size: 1000
  timeout_seconds: 30
  priority_enabled: true
  deduplication_enabled: true
  max_retries: 3
  
session_pool:
  min_connections: 2
  max_connections: 10
  target_connections: 5
  auto_scale: true
  health_check_interval_seconds: 60
  idle_timeout_seconds: 300
  
cache:
  enabled: true
  default_ttl_seconds: 300
  max_size: 1000
  levels:
    l1:
      type: "memory"
      max_size: 1000
    l2:
      type: "redis"
      url: "redis://localhost:6379"
      enabled: false
      
rate_limiting:
  enabled: true
  requests_per_second: 10
  burst_capacity: 100
  
circuit_breaker:
  enabled: true
  failure_threshold: 5
  recovery_timeout_seconds: 30
  
logging:
  level: "INFO"
  format: "json"
  file: "/var/log/mcp-bridge.log"
  structured: true
  
monitoring:
  prometheus_enabled: true
  prometheus_port: 9090
  health_check_path: "/health"
```

---

## ðŸ§ª **TESTS & QUALITÃ‰ AVANCÃ‰S**

### **StratÃ©gie de Tests ComplÃ¨te**
```python
# Tests unitaires
tests/unit/
â”œâ”€â”€ test_queue_manager.py          # File d'attente FIFO
â”œâ”€â”€ test_session_pool.py           # Pool de sessions MCP
â”œâ”€â”€ test_cache_manager.py          # SystÃ¨me de cache multi-niveau
â”œâ”€â”€ test_rate_limiter.py           # Rate limiting
â”œâ”€â”€ test_circuit_breaker.py        # Circuit breaker
â””â”€â”€ test_collision_detection.py    # DÃ©tection conflits

# Tests d'intÃ©gration
tests/integration/
â”œâ”€â”€ test_http_endpoints.py         # API REST complÃ¨te
â”œâ”€â”€ test_mcp_communication.py      # Communication MCP
â”œâ”€â”€ test_concurrent_requests.py    # RequÃªtes simultanÃ©es
â”œâ”€â”€ test_error_scenarios.py        # ScÃ©narios d'erreur
â””â”€â”€ test_failover.py               # Tests de basculement

# Tests de performance
tests/performance/
â”œâ”€â”€ test_load_testing.py           # Tests de charge graduels
â”œâ”€â”€ test_stress_testing.py         # Tests de stress extrÃªme
â”œâ”€â”€ test_memory_usage.py           # Profiling mÃ©moire
â””â”€â”€ test_latency_benchmarks.py     # Benchmarks latence

# Tests end-to-end
tests/e2e/
â”œâ”€â”€ test_full_workflow.py          # Workflows complets
â”œâ”€â”€ test_real_world_scenarios.py   # ScÃ©narios rÃ©els
â””â”€â”€ test_integration_lm_studio.py  # Tests avec LM Studio
```

### **Monitoring & ObservabilitÃ©**
```python
# MÃ©triques dÃ©taillÃ©es
metrics = {
    # Performance
    "request_duration_histogram": Histogram(
        "Duration of HTTP requests",
        buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0]
    ),
    "queue_wait_time": Histogram("Time spent waiting in queue"),
    "mcp_response_time": Histogram("MCP command response time"),
    
    # Throughput
    "requests_total": Counter("Total HTTP requests", ["method", "endpoint"]),
    "requests_per_second": Gauge("Current requests per second"),
    
    # Queue & Sessions
    "queue_size_current": Gauge("Current queue size"),
    "queue_size_max": Gauge("Maximum queue size reached"),
    "active_sessions": Gauge("Active MCP sessions"),
    "session_pool_utilization": Gauge("Session pool utilization %"),
    
    # Cache
    "cache_hits_total": Counter("Cache hits", ["level"]),
    "cache_misses_total": Counter("Cache misses", ["level"]),
    "cache_evictions_total": Counter("Cache evictions", ["reason"]),
    
    # Errors
    "errors_total": Counter("Total errors", ["type", "severity"]),
    "circuit_breaker_state": Gauge("Circuit breaker state (0=closed, 1=open)"),
    "rate_limit_exceeded_total": Counter("Rate limit exceeded events")
}
```

---

## ðŸš¢ **DÃ‰PLOIEMENT & PRODUCTION**

### **Container Strategy**
```dockerfile
# Dockerfile optimisÃ©
FROM python:3.11-alpine
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 3003 9090
HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
  CMD curl -f http://localhost:3003/health || exit 1
CMD ["python", "-m", "mcp_bridge"]
```

### **Docker Compose Stack**
```yaml
version: '3.8'
services:
  mcp-bridge:
    build: .
    ports:
      - "3003:3003"
      - "9090:9090"
    environment:
      - HASS_URL=http://homeassistant:8123
      - HASS_TOKEN=${HASS_TOKEN}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis
      - homeassistant
    restart: unless-stopped
    
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    restart: unless-stopped
    
  prometheus:
    image: prom/prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped
```

---

**Cette architecture garantit :**
- âœ… **SÃ©curitÃ© Maximale** : Files d'attente prÃ©venant tous conflits
- âœ… **Performance Optimale** : Cache intelligent et pooling adaptatif
- âœ… **Robustesse ExtrÃªme** : Circuit breakers et fallbacks
- âœ… **ScalabilitÃ© Native** : Auto-scaling et load balancing
- âœ… **ObservabilitÃ© ComplÃ¨te** : MÃ©triques, logs et monitoring
- âœ… **Production Ready** : Containerisation et CI/CD